{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tweepy \n",
    "from dotenv import load_dotenv \n",
    "import re \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import pandas as pd \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/toygunozyurek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/toygunozyurek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('turkish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.getenv('consumer_key')\n",
    "consumer_key_secret = os.getenv('consumer_key_secret')\n",
    "twitter_acces_token = os.getenv('twitter_acces_token')\n",
    "twitter_acces_token_secret = os.getenv('twitter_acces_token_secret')\n",
    "\n",
    "\n",
    "auth = (tweepy.OAuthHandler(consumer_key, consumer_key_secret, twitter_acces_token, twitter_acces_token_secret))\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding = 'ISO-8859-1',header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   0       1600000 non-null  int64 \n",
      " 1   1       1600000 non-null  int64 \n",
      " 2   2       1600000 non-null  object\n",
      " 3   3       1600000 non-null  object\n",
      " 4   4       1600000 non-null  object\n",
      " 5   5       1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = df[[0,5]] # 0 is the sentiment and 5 is the tweet \n",
    "df.columns = ['sentiment','tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment']  = df['sentiment'].map({0:0,4:1,2:2}) # Replace 4 with 1 and 0 with 0 and 2 with 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweet'] = df['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Temizlenmi≈ü tweetler ve etiketler\n",
    "tweets = df['cleaned_tweet'].tolist()\n",
    "labels = df['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = Tokenizer(num_words = 5000, split = ' ')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(max(len(x) for x in X_train), max(len(x) for x in X_test))\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train)\n",
    "X_test = pad_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 5000,output_dim = 128, input_length = max_len))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(SpatialDropout1D(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(150,dropout=0.2,recurrent_dropout = 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 40, 128)           640000    \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spati  (None, 40, 128)           0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 150)               167400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 453       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 807853 (3.08 MB)\n",
      "Trainable params: 807853 (3.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'list'>\n",
      "<class 'numpy.ndarray'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), type(y_train))\n",
    "print(type(X_test), type(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Hedef verilerin ≈üekillerini kontrol edin ve tek boyutlu hale getirin\n",
    "if y_train.ndim != 1:\n",
    "    y_train = y_train.reshape(-1)\n",
    "if y_test.ndim != 1:\n",
    "    y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), type(y_train))\n",
    "print(type(X_test), type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 40)\n",
      "(320000, 40)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20000/20000 - 1916s - loss: 0.4301 - accuracy: 0.7992 - val_loss: 0.4056 - val_accuracy: 0.8138 - 1916s/epoch - 96ms/step\n",
      "Epoch 2/10\n",
      "20000/20000 - 1955s - loss: 0.3996 - accuracy: 0.8175 - val_loss: 0.3959 - val_accuracy: 0.8194 - 1955s/epoch - 98ms/step\n",
      "Epoch 3/10\n",
      "20000/20000 - 1907s - loss: 0.3872 - accuracy: 0.8246 - val_loss: 0.3913 - val_accuracy: 0.8222 - 1907s/epoch - 95ms/step\n",
      "Epoch 4/10\n",
      "20000/20000 - 1976s - loss: 0.3796 - accuracy: 0.8287 - val_loss: 0.3884 - val_accuracy: 0.8236 - 1976s/epoch - 99ms/step\n",
      "Epoch 5/10\n",
      "20000/20000 - 2007s - loss: 0.3739 - accuracy: 0.8318 - val_loss: 0.3886 - val_accuracy: 0.8242 - 2007s/epoch - 100ms/step\n",
      "Epoch 6/10\n",
      "20000/20000 - 2009s - loss: 0.3702 - accuracy: 0.8337 - val_loss: 0.3884 - val_accuracy: 0.8249 - 2009s/epoch - 100ms/step\n",
      "Epoch 7/10\n",
      "20000/20000 - 2017s - loss: 0.3674 - accuracy: 0.8355 - val_loss: 0.3894 - val_accuracy: 0.8243 - 2017s/epoch - 101ms/step\n",
      "Epoch 8/10\n",
      "20000/20000 - 1968s - loss: 0.3651 - accuracy: 0.8364 - val_loss: 0.3894 - val_accuracy: 0.8252 - 1968s/epoch - 98ms/step\n",
      "Epoch 9/10\n",
      "20000/20000 - 1905s - loss: 0.3635 - accuracy: 0.8374 - val_loss: 0.3915 - val_accuracy: 0.8248 - 1905s/epoch - 95ms/step\n",
      "Epoch 10/10\n",
      "20000/20000 - 1886s - loss: 0.3624 - accuracy: 0.8381 - val_loss: 0.3898 - val_accuracy: 0.8258 - 1886s/epoch - 94ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1791a7ed0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L√ºtfen analiz etmek istediƒüiniz tweetleri girin (bitirmek i√ßin 'q' tu≈üuna basƒ±n):\n"
     ]
    }
   ],
   "source": [
    "new_tweets = []\n",
    "print(\"L√ºtfen analiz etmek istediƒüiniz tweetleri girin (bitirmek i√ßin 'q' tu≈üuna basƒ±n):\")\n",
    "while True:\n",
    "    tweet = input(\"Tweet: \")\n",
    "    if tweet.lower() == 'q':\n",
    "        break\n",
    "    new_tweets.append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 205ms/step\n"
     ]
    }
   ],
   "source": [
    "preprocessed_new_tweets = [preprocess_text(tweet) for tweet in new_tweets]\n",
    "\n",
    "# Yeni tweetleri sayƒ±sal deƒüerlere d√∂n√º≈üt√ºrme\n",
    "new_sequences = tokenizer.texts_to_sequences(preprocessed_new_tweets)\n",
    "\n",
    "# Yeni tweetleri pad etme\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_len)\n",
    "\n",
    "# Tahmin yapma\n",
    "predictions = model.predict(new_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: bug√ºn √ßok iyi hissediyorum.\n",
      "Predicted Sentiment: Neutral\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
    "for tweet, prediction in zip(new_tweets, predictions):\n",
    "    sentiment = sentiment_labels[prediction.argmax()]\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
